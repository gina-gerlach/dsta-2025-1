{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 4 - Task 3: Data Analysis with Jupyter Notebook\n",
    "\n",
    "**Authors:** Gina Gerlach & Sven Regli\n",
    "\n",
    "This notebook performs data analysis on the MNIST dataset using NumPy, Matplotlib, and Scikit-Learn.\n",
    "\n",
    "## Objectives:\n",
    "1. Load MNIST data into NumPy arrays\n",
    "2. Analyze data structure and characteristics\n",
    "3. Create visualizations (histograms, distributions)\n",
    "4. Analyze model predictions from Task 2\n",
    "5. Create a Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import  libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Loading and Exploring MNIST Data\n",
    "\n",
    "We'll load the MNIST dataset and examine its structure as NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MNIST Dataset Structure\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training data shape: {x_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test data shape: {x_test.shape}\")\n",
    "print(f\"Test labels shape: {y_test.shape}\")\n",
    "print(f\"\\nData type: {x_train.dtype}\")\n",
    "print(f\"Label type: {y_train.dtype}\")\n",
    "print(f\"\\nPixel value range: [{x_train.min()}, {x_train.max()}]\")\n",
    "print(f\"Label range: [{y_train.min()}, {y_train.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Array Dimensions\n",
    "\n",
    "Each image in the MNIST dataset is represented as a NumPy array with dimensions:\n",
    "- **Width:** 28 pixels\n",
    "- **Height:** 28 pixels\n",
    "- **Channels:** 1 (grayscale)\n",
    "\n",
    "When we load a single image, we get an array of shape `(28, 28)` where each element represents a pixel's grayscale intensity (0-255)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single image to examine\n",
    "sample_image = x_train[0]\n",
    "sample_label = y_train[0]\n",
    "\n",
    "print(f\"Single image shape: {sample_image.shape}\")\n",
    "print(f\"Single image label: {sample_label}\")\n",
    "print(f\"\\nFirst 5x5 pixel values of the image:\\n{sample_image[:5, :5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing Sample Images\n",
    "\n",
    "Let's visualize some sample images to understand what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 10 random images from the training set\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('Random MNIST Training Samples', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Randomly select 10 indices\n",
    "random_indices = np.random.choice(len(x_train), 10, replace=False)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    image_idx = random_indices[idx]\n",
    "    ax.imshow(x_train[image_idx], cmap='gray')\n",
    "    ax.set_title(f'Label: {y_train[image_idx]}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Pixel Value Distribution Analysis\n",
    "\n",
    "Since MNIST images are grayscale (single channel), we'll analyze the distribution of pixel intensities across the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten all training images into a 1D array for histogram analysis\n",
    "all_pixels = x_train.flatten()\n",
    "\n",
    "print(f\"Total number of pixels in training set: {len(all_pixels):,}\")\n",
    "print(f\"Mean pixel value: {all_pixels.mean():.2f}\")\n",
    "print(f\"Std dev of pixel values: {all_pixels.std():.2f}\")\n",
    "print(f\"Median pixel value: {np.median(all_pixels):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histogram of pixel intensities\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram with all 256 bins\n",
    "ax1.hist(all_pixels, bins=256, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax1.set_xlabel('Pixel Intensity', fontsize=12)\n",
    "ax1.set_ylabel('Frequency', fontsize=12)\n",
    "ax1.set_title('Distribution of Pixel Intensities (All Training Images)', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Histogram with log scale to see distribution better\n",
    "ax2.hist(all_pixels, bins=256, color='coral', edgecolor='black', alpha=0.7)\n",
    "ax2.set_xlabel('Pixel Intensity', fontsize=12)\n",
    "ax2.set_ylabel('Frequency (log scale)', fontsize=12)\n",
    "ax2.set_title('Distribution of Pixel Intensities (Log Scale)', fontsize=14, fontweight='bold')\n",
    "ax2.set_yscale('log')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- Most pixels are either 0 (black background) or 255 (white foreground)\")\n",
    "print(\"- This is typical for binary-style handwritten digit images\")\n",
    "print(\"- Gray values (edges/anti-aliasing) are less frequent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Label Distribution Analysis\n",
    "\n",
    "Let's analyze how the digits are distributed in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count occurrences of each digit\n",
    "train_counts = np.bincount(y_train)\n",
    "test_counts = np.bincount(y_test)\n",
    "\n",
    "# Create bar plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "digits = np.arange(10)\n",
    "\n",
    "# Training set distribution\n",
    "ax1.bar(digits, train_counts, color='skyblue', edgecolor='black', alpha=0.8)\n",
    "ax1.set_xlabel('Digit', fontsize=12)\n",
    "ax1.set_ylabel('Count', fontsize=12)\n",
    "ax1.set_title('Training Set Label Distribution', fontsize=14, fontweight='bold')\n",
    "ax1.set_xticks(digits)\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, count in enumerate(train_counts):\n",
    "    ax1.text(i, count + 100, str(count), ha='center', fontsize=10)\n",
    "\n",
    "# Test set distribution\n",
    "ax2.bar(digits, test_counts, color='lightcoral', edgecolor='black', alpha=0.8)\n",
    "ax2.set_xlabel('Digit', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.set_title('Test Set Label Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(digits)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, count in enumerate(test_counts):\n",
    "    ax2.text(i, count + 20, str(count), ha='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLabel Distribution Summary:\")\n",
    "print(f\"Training set - Min: {train_counts.min()}, Max: {train_counts.max()}, Mean: {train_counts.mean():.1f}\")\n",
    "print(f\"Test set - Min: {test_counts.min()}, Max: {test_counts.max()}, Mean: {test_counts.mean():.1f}\")\n",
    "print(\"\\nThe dataset is fairly balanced across all digits!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Average Digit Visualization\n",
    "\n",
    "Let's compute and visualize the average image for each digit class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average image for each digit\n",
    "avg_digits = np.zeros((10, 28, 28))\n",
    "\n",
    "for digit in range(10):\n",
    "    # Find all images of this digit\n",
    "    digit_images = x_train[y_train == digit]\n",
    "    # Compute mean across all images\n",
    "    avg_digits[digit] = digit_images.mean(axis=0)\n",
    "\n",
    "# Visualize average digits\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "fig.suptitle('Average Image for Each Digit Class', fontsize=16, fontweight='bold')\n",
    "\n",
    "for digit, ax in enumerate(axes.flat):\n",
    "    ax.imshow(avg_digits[digit], cmap='hot')\n",
    "    ax.set_title(f'Digit {digit}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"These average images show the 'typical' appearance of each digit.\")\n",
    "print(\"Notice how some digits (like 1) are more consistent, while others (like 2) show more variation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Loading Model and Making Predictions\n",
    "\n",
    "Now we'll load a trained model from Task 2 and generate predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load model from Weights & Biases\nimport os\nimport glob\nimport wandb\nfrom dotenv import load_dotenv\n\n# Load environment variables from .env file\nload_dotenv()\n\n# Login to W&B\nwandb_token = os.getenv(\"WANDB_TOKEN\")\nif wandb_token:\n    wandb.login(key=wandb_token)\n    print(\"Logged in to W&B successfully!\")\nelse:\n    print(\"Warning: WANDB_TOKEN not found. Attempting to use cached credentials.\")\n\ntry:\n    # Initialize W&B API\n    api = wandb.Api()\n    \n    # Get all runs from the project\n    # Replace 'YOUR_USERNAME' with your actual W&B username\n    project_path = \"mnist_task2\"\n    print(f\"\\nFetching runs from project: {project_path}\")\n    \n    runs = api.runs(project_path)\n    \n    if len(runs) == 0:\n        print(\"No runs found in the project. Falling back to local model loading...\")\n        raise Exception(\"No W&B runs found\")\n    \n    # Find the run with the best validation accuracy\n    best_run = None\n    best_accuracy = 0\n    \n    for run in runs:\n        # Check if run has accuracy metric\n        acc = run.summary.get('accuracy', 0) or run.summary.get('val_accuracy', 0)\n        if acc > best_accuracy:\n            best_accuracy = acc\n            best_run = run\n    \n    print(f\"\\nBest run found: {best_run.name} (ID: {best_run.id})\")\n    print(f\"Best accuracy: {best_accuracy * 100:.2f}%\")\n    print(f\"Config: {best_run.config}\")\n    \n    # Get model artifacts from the best run\n    artifacts = best_run.logged_artifacts()\n    model_artifacts = [a for a in artifacts if a.type == 'model']\n    \n    if len(model_artifacts) == 0:\n        print(\"No model artifacts found. Falling back to local model loading...\")\n        raise Exception(\"No model artifacts\")\n    \n    # Download the model artifact\n    model_artifact = model_artifacts[0]\n    print(f\"\\nDownloading model artifact: {model_artifact.name}\")\n    artifact_dir = model_artifact.download()\n    \n    # Find the .keras file in the downloaded directory\n    model_files = glob.glob(os.path.join(artifact_dir, '*.keras')) + glob.glob(os.path.join(artifact_dir, '*.h5'))\n    \n    if model_files:\n        model_path = model_files[0]\n        print(f\"\\nLoading model from W&B: {model_path}\")\n        model = keras.models.load_model(model_path)\n        print(\"Model loaded successfully from W&B!\\n\")\n        model.summary()\n    else:\n        print(\"No model file found in artifact. Falling back to local model loading...\")\n        raise Exception(\"No model file in artifact\")\n        \nexcept Exception as e:\n    print(f\"\\nError loading from W&B: {e}\")\n    print(\"Attempting to load from local 'models' directory instead...\\n\")\n    \n    # Fallback: Load from local directory\n    model_dir = 'models'\n    if os.path.exists(model_dir):\n        model_files = glob.glob(os.path.join(model_dir, '*.keras')) + glob.glob(os.path.join(model_dir, '*.h5'))\n        if model_files:\n            # Use the first model found\n            model_path = model_files[0]\n            print(f\"Loading model from local file: {model_path}\")\n            model = keras.models.load_model(model_path)\n            print(\"Model loaded successfully from local file!\\n\")\n            model.summary()\n        else:\n            print(\"No model files found. Please train a model first using train_and_save_wandb.py\")\n            model = None\n    else:\n        print(\"Models directory not found. Please train a model first.\")\n        model = None"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data (normalize and reshape)\n",
    "if model is not None:\n",
    "    # Normalize pixel values\n",
    "    x_test_norm = x_test.astype(\"float32\") / 255.0\n",
    "    # Reshape to add channel dimension\n",
    "    x_test_norm = np.expand_dims(x_test_norm, -1)\n",
    "    \n",
    "    print(f\"Test data shape after preprocessing: {x_test_norm.shape}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    print(\"\\nMaking predictions on test set...\")\n",
    "    predictions_proba = model.predict(x_test_norm, verbose=0)\n",
    "    predictions = np.argmax(predictions_proba, axis=1)\n",
    "    \n",
    "    print(f\"Predictions shape: {predictions.shape}\")\n",
    "    print(f\"Probability matrix shape: {predictions_proba.shape}\")\n",
    "    print(f\"\\nFirst 10 predictions: {predictions[:10]}\")\n",
    "    print(f\"First 10 ground truth: {y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Model Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    \n",
    "    # Calculate number of correct and incorrect predictions\n",
    "    correct = np.sum(predictions == y_test)\n",
    "    incorrect = np.sum(predictions != y_test)\n",
    "    \n",
    "    print(f\"\\nCorrect predictions: {correct}/{len(y_test)}\")\n",
    "    print(f\"Incorrect predictions: {incorrect}/{len(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Correct and Incorrect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Find indices of correct and incorrect predictions\n",
    "    correct_indices = np.where(predictions == y_test)[0]\n",
    "    incorrect_indices = np.where(predictions != y_test)[0]\n",
    "    \n",
    "    # Display some correct predictions\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "    fig.suptitle('Sample Correct Predictions', fontsize=16, fontweight='bold', color='green')\n",
    "    \n",
    "    sample_correct = np.random.choice(correct_indices, 10, replace=False)\n",
    "    \n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        img_idx = sample_correct[idx]\n",
    "        ax.imshow(x_test[img_idx], cmap='gray')\n",
    "        ax.set_title(f'True: {y_test[img_idx]}, Pred: {predictions[img_idx]}', \n",
    "                     fontsize=10, color='green')\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display some incorrect predictions\n",
    "    if len(incorrect_indices) > 0:\n",
    "        fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "        fig.suptitle('Sample Incorrect Predictions', fontsize=16, fontweight='bold', color='red')\n",
    "        \n",
    "        num_incorrect_to_show = min(10, len(incorrect_indices))\n",
    "        sample_incorrect = np.random.choice(incorrect_indices, num_incorrect_to_show, replace=False)\n",
    "        \n",
    "        for idx, ax in enumerate(axes.flat):\n",
    "            if idx < num_incorrect_to_show:\n",
    "                img_idx = sample_incorrect[idx]\n",
    "                ax.imshow(x_test[img_idx], cmap='gray')\n",
    "                confidence = predictions_proba[img_idx][predictions[img_idx]] * 100\n",
    "                ax.set_title(f'True: {y_test[img_idx]}, Pred: {predictions[img_idx]}\\nConf: {confidence:.1f}%', \n",
    "                             fontsize=9, color='red')\n",
    "                ax.axis('off')\n",
    "            else:\n",
    "                ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No incorrect predictions found! Perfect accuracy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Confusion Matrix\n",
    "\n",
    "A confusion matrix shows which digits are frequently confused with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    # Display confusion matrix as heatmap\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=np.arange(10))\n",
    "    disp.plot(cmap='Blues', ax=ax, colorbar=True)\n",
    "    \n",
    "    ax.set_title('Confusion Matrix - MNIST Test Set', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nConfusion Matrix Interpretation:\")\n",
    "    print(\"- Diagonal elements: correct predictions\")\n",
    "    print(\"- Off-diagonal elements: misclassifications\")\n",
    "    print(\"- Darker blue = higher count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Confusion Matrix\n",
    "\n",
    "Let's also look at the confusion matrix normalized by true labels to see the proportion of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Compute normalized confusion matrix\n",
    "    cm_normalized = confusion_matrix(y_test, predictions, normalize='true')\n",
    "    \n",
    "    # Display normalized confusion matrix\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.3f', cmap='RdYlGn_r', \n",
    "                xticklabels=np.arange(10), yticklabels=np.arange(10),\n",
    "                cbar_kws={'label': 'Proportion'}, ax=ax)\n",
    "    \n",
    "    ax.set_title('Normalized Confusion Matrix (by True Label)', fontsize=16, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12)\n",
    "    ax.set_ylabel('True Label', fontsize=12)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nNormalized Confusion Matrix shows the proportion of predictions for each true class.\")\n",
    "    print(\"Values close to 1.0 on the diagonal indicate high accuracy for that digit.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Classification Report\n",
    "\n",
    "Let's generate a detailed classification report showing precision, recall, and F1-score for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, predictions, target_names=[f'Digit {i}' for i in range(10)])\n",
    "    print(\"Classification Report:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(report)\n",
    "    \n",
    "    print(\"\\nMetrics Explanation:\")\n",
    "    print(\"- Precision: Of all predictions for a digit, how many were correct?\")\n",
    "    print(\"- Recall: Of all actual instances of a digit, how many were found?\")\n",
    "    print(\"- F1-Score: Harmonic mean of precision and recall\")\n",
    "    print(\"- Support: Number of true instances for each digit in the test set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Per-Class Accuracy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model is not None:\n",
    "    # Calculate per-class accuracy\n",
    "    per_class_accuracy = []\n",
    "    \n",
    "    for digit in range(10):\n",
    "        # Find all instances of this digit\n",
    "        digit_mask = y_test == digit\n",
    "        digit_predictions = predictions[digit_mask]\n",
    "        digit_true = y_test[digit_mask]\n",
    "        \n",
    "        # Calculate accuracy for this digit\n",
    "        accuracy = np.mean(digit_predictions == digit_true)\n",
    "        per_class_accuracy.append(accuracy)\n",
    "    \n",
    "    # Plot per-class accuracy\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    colors = ['red' if acc < 0.95 else 'green' for acc in per_class_accuracy]\n",
    "    bars = ax.bar(range(10), per_class_accuracy, color=colors, edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    ax.set_xlabel('Digit', fontsize=12)\n",
    "    ax.set_ylabel('Accuracy', fontsize=12)\n",
    "    ax.set_title('Per-Class Accuracy on Test Set', fontsize=14, fontweight='bold')\n",
    "    ax.set_xticks(range(10))\n",
    "    ax.set_ylim([0, 1.05])\n",
    "    ax.axhline(y=0.95, color='orange', linestyle='--', label='95% threshold', linewidth=2)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, acc) in enumerate(zip(bars, per_class_accuracy)):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                f'{acc*100:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    best_digit = np.argmax(per_class_accuracy)\n",
    "    worst_digit = np.argmin(per_class_accuracy)\n",
    "    \n",
    "    print(f\"\\nBest performing digit: {best_digit} ({per_class_accuracy[best_digit]*100:.2f}%)\")\n",
    "    print(f\"Worst performing digit: {worst_digit} ({per_class_accuracy[worst_digit]*100:.2f}%)\")\n",
    "    print(f\"Overall test accuracy: {np.mean(per_class_accuracy)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Data Structure:**\n",
    "   - MNIST images are 28Ã—28 grayscale images represented as NumPy arrays\n",
    "   - Pixel values range from 0 (black) to 255 (white)\n",
    "   - The dataset is well-balanced across all 10 digit classes\n",
    "\n",
    "2. **Pixel Distribution:**\n",
    "   - Most pixels are either 0 (background) or 255 (foreground)\n",
    "   - This bimodal distribution is typical for binary-style images\n",
    "   - Gray values appear at edges due to anti-aliasing\n",
    "\n",
    "3. **Model Performance:**\n",
    "   - The trained CNN achieves high accuracy on the test set\n",
    "   - The confusion matrix reveals which digits are most commonly confused\n",
    "   - Some digits (like 1) are easier to classify than others (like 8 or 9)\n",
    "\n",
    "4. **NumPy for Data Analysis:**\n",
    "   - NumPy arrays provide efficient storage and manipulation of image data\n",
    "   - Array operations enable quick statistical analysis across the entire dataset\n",
    "   - Shape manipulation (flatten, reshape, expand_dims) is crucial for preprocessing\n",
    "\n",
    "5. **Visualization Insights:**\n",
    "   - Matplotlib enables understanding of data distribution and model behavior\n",
    "   - Confusion matrices reveal systematic misclassification patterns\n",
    "   - Average digit images show the \"canonical\" representation of each class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsta-2025-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}